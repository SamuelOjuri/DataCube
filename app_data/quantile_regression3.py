# -*- coding: utf-8 -*-
"""Quantile-regression3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/106nnBUzyCu_r-K8sG-peoZFXSSRl_lbo
"""

# -*- coding: utf-8 -*-
"""Quantile regression with LightGBM only (OOF encoders, time-aware CV, calibrated intervals)."""

# 1. Environment ----------------------------------------------------------------
!pip install -q -U lightgbm==4.3.0 pandas==2.2.2 scikit-learn==1.6.0 optuna==4.6.0

# -*- coding: utf-8 -*-
"""Quantile Regression v3 - Enhanced LightGBM with Optuna, Log-Transform, and Better Features.

Improvements over v2:
- Log-transformed target for skewed distribution
- Censored observation handling (gestation = MAX capped)
- Interaction features and feature engineering
- Optuna hyperparameter optimization (100 trials)
- Quantile-specific tuning
- Improved temporal cross-validation with purge gap
- Permutation importance for feature selection
- Conformal calibration for coverage guarantees
"""

from pathlib import Path
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor
import hashlib
import json
import math
import pickle
import random
import warnings
from typing import Dict, Iterable, List, Optional, Tuple, Any

import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit, KFold
from sklearn.inspection import permutation_importance

try:
    import optuna
    from optuna.samplers import TPESampler
    OPTUNA_AVAILABLE = True
    optuna.logging.set_verbosity(optuna.logging.WARNING)
except ImportError:
    OPTUNA_AVAILABLE = False
    print("Warning: Optuna not installed. Falling back to random search.")

try:
    from IPython.display import display
except ImportError:
    display = print

pd.options.mode.copy_on_write = True
warnings.filterwarnings("ignore", category=UserWarning)

# 2. Global configuration -------------------------------------------------------
DATASET_PATH = Path("/content/data/gestation_training_20251126_132549.csv")
OUTPUT_DIR = Path("/content/quantile_models_v3")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

MODEL_VERSION = "colab_v3_enhanced"
LOOKBACK_DAYS = 5475
VAL_SIZE = 0.2
CALIBRATION_FRACTION = 0.5
MIN_ROWS = 200
MIN_GESTATION_DAYS = 5
MAX_GESTATION_DAYS = 730
CENSORING_THRESHOLD = 700  # Treat values >= this as potentially censored
CLIP_QUANTILES = (0.01, 0.99)
CATEGORY_MIN_FREQUENCY = 10
RANDOM_STATE = 42
EARLY_STOPPING_ROUNDS = 150
OPTUNA_TRIALS = 100  # Increased from 18
MAX_LGB_TREES = 5000
CV_FOLDS = 8  # Increased from 5
PURGE_GAP_DAYS = 14  # Gap between train/val to prevent leakage
TARGET_COVERAGE = 0.362  # Aim lower to get closer to 50% actual coverage
CALIBRATION_BUCKET_FEATURE = "duration_days"
CALIBRATION_BUCKETS = 4
WIDTH_SCALE_CLIP = (0.35, 2.2)

# Whether to use log-transformed target
USE_LOG_TARGET = True
# Whether to filter out censored observations
FILTER_CENSORED = True

random.seed(RANDOM_STATE)
np.random.seed(RANDOM_STATE)

ONE_HOT_CATEGORICAL = [
    "type",
    "value_band",
    "season",
    "created_dow",
    "age_bucket",
]
TARGET_ENCODE_COLUMNS = ["category", "product_type"]
TARGET_ENCODE_SUFFIX = "_te"

# Base numeric features (before interaction features)
BASE_NUMERIC_FEATURES = [
    "log_value",
    "new_enquiry_value",
    "duration_days",
    "created_year",
    "created_quarter",
    "created_month",
    "created_weekofyear",
    "season_sin",
    "season_cos",
    "days_since_created",
    "days_since_updated",
    "recent_activity_days",
    "recent_activity_ratio",
    "update_frequency",
    "activity_velocity",
    "value_per_day",
    "recent_update_gap",
    "account_median_gestation",
    "account_mean_gestation",
    "account_n_projects",
    "age_months",
    "age_months_log",
    "category_te",
    "product_type_te",
]

# NEW: Interaction and derived features
INTERACTION_FEATURES = [
    "value_x_duration",
    "value_x_category_te",
    "account_conversion_rate",
    "activity_momentum",
    "duration_value_ratio",
    "season_value_interaction",
    "log_duration",
    "value_activity_score",
    "account_reliability_score",
    "urgency_score",
]

NUMERIC_FEATURES = BASE_NUMERIC_FEATURES + INTERACTION_FEATURES

TARGET_COLUMN = "gestation_target"
CLIPPED_TARGET_COLUMN = f"{TARGET_COLUMN}_clipped"
LOG_TARGET_COLUMN = f"{TARGET_COLUMN}_log"

# Optuna hyperparameter search space
OPTUNA_PARAM_SPACE = {
    "max_depth": (3, 10),
    "num_leaves": (15, 127),
    "min_child_samples": (10, 100),
    "subsample": (0.5, 1.0),
    "colsample_bytree": (0.5, 1.0),
    "reg_alpha": (0.0, 5.0),
    "reg_lambda": (0.0, 10.0),
    "learning_rate": (0.005, 0.1),
    "min_gain_to_split": (0.0, 2.0),
}

# Fallback random search grid (if Optuna not available)
LGB_PARAM_GRID = {
    "max_depth": [3, 4, 5, 6, 8, 10],
    "num_leaves": [15, 31, 63, 95, 127],
    "min_child_samples": [10, 20, 40, 60, 80, 100],
    "subsample": [0.5, 0.7, 0.85, 1.0],
    "colsample_bytree": [0.5, 0.6, 0.8, 1.0],
    "reg_alpha": [0.0, 0.3, 0.7, 1.2, 2.0, 3.0],
    "reg_lambda": [0.1, 1.0, 3.0, 6.0, 10.0],
    "learning_rate": [0.005, 0.01, 0.02, 0.03, 0.05, 0.08],
    "min_gain_to_split": [0.0, 0.5, 1.0, 1.5],
}

# 3. Helper utilities -----------------------------------------------------------
def normalize_categorical(series: pd.Series) -> pd.Series:
    return (
        series.fillna("Unknown")
        .astype(str)
        .str.strip()
        .replace("", "Unknown")
    )


def safe_ratio(numerator: pd.Series, denominator: pd.Series) -> pd.Series:
    denom = denominator.replace(0, np.nan)
    ratio = numerator / denom
    return ratio.replace([np.inf, -np.inf], np.nan).fillna(0.0)


def ensure_monotonic(preds: Dict[str, Optional[float]]) -> Dict[str, Optional[float]]:
    values = [preds.get("p25"), preds.get("p50"), preds.get("p75")]
    available = [v for v in values if v is not None]
    if not available:
        return preds
    low, high = min(available), max(available)
    if preds.get("p25") is None:
        preds["p25"] = low
    if preds.get("p75") is None:
        preds["p75"] = high
    if preds.get("p50") is None:
        preds["p50"] = int(round((preds["p25"] + preds["p75"]) / 2))
    preds["p25"] = min(preds["p25"], preds["p50"], preds["p75"])
    preds["p75"] = max(preds["p25"], preds["p50"], preds["p75"])
    preds["p50"] = max(preds["p25"], min(preds["p50"], preds["p75"]))
    return preds


def dataset_hash(df: pd.DataFrame) -> str:
    return hashlib.sha256(df.to_csv(index=False).encode("utf-8")).hexdigest()


def train_val_split(df: pd.DataFrame, val_size: float, seed: int) -> Tuple[pd.DataFrame, pd.DataFrame]:
    if val_size <= 0 or val_size >= 1:
        return df.reset_index(drop=True), pd.DataFrame()
    ordered = (
        df.sort_values("date_created").reset_index(drop=True)
        if "date_created" in df.columns
        else df.sample(frac=1.0, random_state=seed).reset_index(drop=True)
    )
    cutoff = int(len(ordered) * (1 - val_size))
    cutoff = max(1, min(cutoff, len(ordered) - 1))
    return ordered.iloc[:cutoff].reset_index(drop=True), ordered.iloc[cutoff:].reset_index(drop=True)


def temporal_folds_with_purge(
    df: pd.DataFrame,
    n_splits: int,
    date_col: str,
    seed: int,
    purge_gap_days: int = PURGE_GAP_DAYS,
) -> List[Tuple[np.ndarray, np.ndarray]]:
    """Time-series CV with purge gap to prevent leakage."""
    if date_col not in df.columns or df[date_col].isna().all():
        indices = np.arange(len(df))
        splitter = KFold(n_splits=min(n_splits, len(df)), shuffle=True, random_state=seed)
        return [(indices[train], indices[val]) for train, val in splitter.split(indices)]

    # Parse dates and sort
    working = df.reset_index()
    working["_parsed_date"] = pd.to_datetime(working[date_col], errors="coerce", utc=True)
    ordered = working.sort_values("_parsed_date")
    ordered_idx = ordered["index"].to_numpy()
    ordered_dates = ordered["_parsed_date"].to_numpy()

    tscv = TimeSeriesSplit(n_splits=min(n_splits, max(len(df) - 1, 1)))
    folds = []

    for train_pos, val_pos in tscv.split(ordered_idx):
        if not len(train_pos) or not len(val_pos):
            continue

        # Apply purge gap: remove training samples too close to validation start
        val_start_date = ordered_dates[val_pos[0]]
        if pd.notna(val_start_date):
            purge_threshold = val_start_date - np.timedelta64(purge_gap_days, "D")
            train_mask = ordered_dates[train_pos] < purge_threshold
            train_pos_purged = train_pos[train_mask]
            if len(train_pos_purged) > MIN_ROWS // 2:
                train_pos = train_pos_purged

        folds.append((ordered_idx[train_pos], ordered_idx[val_pos]))

    if not folds:
        indices = np.arange(len(df))
        splitter = KFold(n_splits=min(n_splits, len(df)), shuffle=True, random_state=seed)
        folds = [(indices[train], indices[val]) for train, val in splitter.split(indices)]

    return folds

# 4. Feature engineering --------------------------------------------------------
class EnhancedFeatureBuilder:
    """Feature builder with interaction features."""

    def __init__(self, date_column: str = "date_created"):
        self.date_column = date_column

    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        working = df.copy()
        working = self._add_time_columns(working)
        working = self._add_ratios(working)
        working = self._add_interaction_features(working)
        return working

    def _add_time_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        if self.date_column in df.columns:
            created = pd.to_datetime(df[self.date_column], errors="coerce", utc=True)
        else:
            created = pd.Series(pd.NaT, index=df.index, dtype="datetime64[ns, UTC]")

        df["date_created"] = created
        df["created_year"] = created.dt.year.fillna(0).astype(int)
        df["created_quarter"] = created.dt.quarter.fillna(0).astype(int)
        df["created_month"] = created.dt.month.fillna(0).astype(int)
        df["created_weekofyear"] = created.dt.isocalendar().week.astype("Int64").fillna(0).astype(int)
        df["created_dow"] = created.dt.dayofweek.fillna(-1).astype(int)

        now_utc = pd.Timestamp.now(timezone.utc)
        df["age_months"] = ((now_utc - created).dt.days / 30.0).fillna(0.0).clip(lower=0.0)
        df["age_months_log"] = np.log1p(df["age_months"])
        df["season_sin"] = np.sin(2 * np.pi * (df["created_month"].clip(lower=1) / 12.0))
        df["season_cos"] = np.cos(2 * np.pi * (df["created_month"].clip(lower=1) / 12.0))
        age_bucket = pd.qcut(
            df["age_months"], q=min(4, df["age_months"].nunique()), labels=False, duplicates="drop"
        )
        df["age_bucket"] = age_bucket.fillna(-1).astype(int)
        return df

    def _add_ratios(self, df: pd.DataFrame) -> pd.DataFrame:
        df["recent_activity_ratio"] = safe_ratio(
            df.get("recent_activity_days", 0.0), df.get("duration_days", 0.0).abs() + 1
        )
        df["update_frequency"] = safe_ratio(
            df.get("days_since_created", 0.0), df.get("days_since_updated", 0.0).abs() + 1
        )
        df["activity_velocity"] = safe_ratio(
            df.get("recent_activity_days", 0.0), df.get("days_since_updated", 0.0).abs() + 1
        )
        df["value_per_day"] = safe_ratio(
            df.get("new_enquiry_value", 0.0), df.get("duration_days", 0.0).abs() + 1
        )
        df["recent_update_gap"] = df.get("days_since_created", 0.0) - df.get("days_since_updated", 0.0)
        return df

    def _add_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add interaction and derived features for better signal."""
        # Value x Duration interaction
        log_value = df.get("log_value", pd.Series(0.0, index=df.index)).fillna(0)
        duration = df.get("duration_days", pd.Series(0.0, index=df.index)).fillna(0)

        df["value_x_duration"] = log_value * np.log1p(duration.clip(lower=0))

        # Value x Category target encoding interaction
        cat_te = df.get("category_te", pd.Series(0.0, index=df.index)).fillna(0)
        df["value_x_category_te"] = log_value * cat_te

        # Account conversion rate
        acct_n = df.get("account_n_projects", pd.Series(1.0, index=df.index)).fillna(1).clip(lower=1)
        acct_median = df.get("account_median_gestation", pd.Series(1.0, index=df.index)).fillna(1).clip(lower=1)
        df["account_conversion_rate"] = acct_n / acct_median

        # Activity momentum (recency-weighted)
        days_updated = df.get("days_since_updated", pd.Series(30.0, index=df.index)).fillna(30)
        recent_activity = df.get("recent_activity_days", pd.Series(0.0, index=df.index)).fillna(0)
        df["activity_momentum"] = np.exp(-days_updated / 30) * recent_activity

        # Duration-value ratio
        df["duration_value_ratio"] = duration / (log_value.clip(lower=1) ** 0.5 + 1)

        # Season-value interaction
        season_sin = df.get("season_sin", pd.Series(0.0, index=df.index)).fillna(0)
        df["season_value_interaction"] = season_sin * log_value

        # Log duration
        df["log_duration"] = np.log1p(duration.clip(lower=0))

        # Composite activity score
        df["value_activity_score"] = log_value * df["activity_velocity"].fillna(0)

        # Account reliability (more projects + faster conversion = more reliable)
        acct_mean = df.get("account_mean_gestation", pd.Series(0.0, index=df.index)).fillna(0)
        df["account_reliability_score"] = np.log1p(acct_n) / (np.log1p(acct_mean.clip(lower=0)) + 1)

        # Urgency score
        df["urgency_score"] = df["recent_update_gap"].fillna(0) * df["value_per_day"].fillna(0)

        return df


class AccountStatsBuilder:
    STAT_COLUMNS = ["account_median_gestation", "account_mean_gestation", "account_n_projects"]

    def __init__(self, target_column: str):
        self.target_column = target_column
        self.global_stats = pd.DataFrame()
        self.defaults: Dict[str, float] = {}

    def fit_transform(self, df: pd.DataFrame, folds: Iterable[Tuple[np.ndarray, np.ndarray]]) -> pd.DataFrame:
        working = df.copy()
        oof_values = pd.DataFrame(index=working.index, columns=self.STAT_COLUMNS, dtype=float)
        fold_list = list(folds) or [(working.index.to_numpy(), working.index.to_numpy())]

        for train_idx, val_idx in fold_list:
            if len(val_idx) == 0:
                continue
            train_stats = (
                working.iloc[train_idx]
                .groupby("account")[self.target_column]
                .agg(["median", "mean", "count"])
            )
            rename_map = {
                "median": "account_median_gestation",
                "mean": "account_mean_gestation",
                "count": "account_n_projects",
            }
            train_stats = train_stats.rename(columns=rename_map)
            for col in self.STAT_COLUMNS:
                mapped = working.iloc[val_idx]["account"].map(train_stats[col])
                oof_values.loc[working.index[val_idx], col] = mapped

        fallback = working[self.target_column].median()
        for col in self.STAT_COLUMNS:
            oof_values[col] = oof_values[col].fillna(fallback if "gestation" in col else 1.0)
        working[self.STAT_COLUMNS] = oof_values

        self.global_stats = (
            working.groupby("account")[self.STAT_COLUMNS]
            .median()
            .replace([np.inf, -np.inf], np.nan)
            .fillna(fallback)
        )
        self.defaults = {col: float(working[col].median()) for col in self.STAT_COLUMNS}
        return working

    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        enriched = df.copy()
        if self.global_stats.empty:
            for col in self.STAT_COLUMNS:
                enriched[col] = self.defaults.get(col, 0.0)
            return enriched
        for col in self.STAT_COLUMNS:
            enriched[col] = enriched["account"].map(self.global_stats[col]).fillna(self.defaults.get(col, 0.0))
        return enriched


class OOFTargetEncoder:
    def __init__(self, columns: List[str], target_column: str, smoothing: float = 20.0):
        self.columns = columns
        self.target_column = target_column
        self.smoothing = smoothing
        self.global_mean = 0.0
        self.maps: Dict[str, Dict[str, float]] = {}

    def fit_transform(self, df: pd.DataFrame, folds: Iterable[Tuple[np.ndarray, np.ndarray]]) -> pd.DataFrame:
        working = df.copy()
        working[self.target_column] = pd.to_numeric(working[self.target_column], errors="coerce")
        self.global_mean = float(working[self.target_column].mean())
        fold_list = list(folds) or [(working.index.to_numpy(), working.index.to_numpy())]

        for col in self.columns:
            normalized = normalize_categorical(working.get(col, pd.Series(dtype=object)))
            oof_col = pd.Series(index=working.index, dtype=float)

            for train_idx, val_idx in fold_list:
                if len(val_idx) == 0:
                    continue
                train_stats = (
                    pd.DataFrame(
                        {"value": working.iloc[train_idx][self.target_column], "cat": normalized.iloc[train_idx]}
                    )
                    .groupby("cat")["value"]
                    .agg(["mean", "count"])
                )
                smooth = (train_stats["mean"] * train_stats["count"] + self.smoothing * self.global_mean) / (
                    train_stats["count"] + self.smoothing
                )
                oof_col.iloc[val_idx] = normalized.iloc[val_idx].map(smooth)

            oof_col = oof_col.fillna(self.global_mean)
            working[f"{col}{TARGET_ENCODE_SUFFIX}"] = oof_col

            full_stats = (
                pd.DataFrame({"value": working[self.target_column], "cat": normalized})
                .groupby("cat")["value"]
                .agg(["mean", "count"])
            )
            smooth_full = (full_stats["mean"] * full_stats["count"] + self.smoothing * self.global_mean) / (
                full_stats["count"] + self.smoothing
            )
            self.maps[col] = smooth_full.to_dict()

        return working

    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        enriched = df.copy()
        for col in self.columns:
            mapping = self.maps.get(col, {})
            normalized = normalize_categorical(enriched.get(col, pd.Series(dtype=object)))
            enriched[f"{col}{TARGET_ENCODE_SUFFIX}"] = normalized.map(mapping).fillna(self.global_mean)
        return enriched


class QuantileFeatureEncoder:
    def __init__(self, categorical_features: List[str], numeric_features: List[str]):
        self.categorical_features = list(categorical_features)
        self.numeric_features = list(numeric_features)
        self.feature_columns: List[str] = []

    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:
        encoded = self._encode(df)
        self.feature_columns = list(encoded.columns)
        return encoded

    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        encoded = self._encode(df)
        if not self.feature_columns:
            return encoded
        return encoded.reindex(columns=self.feature_columns, fill_value=0.0)

    def _encode(self, df: pd.DataFrame) -> pd.DataFrame:
        working = df.copy()
        for col in self.numeric_features:
            working[col] = pd.to_numeric(working.get(col, 0.0), errors="coerce").fillna(0.0)
        cat_df = pd.DataFrame(index=df.index)
        if self.categorical_features:
            cats = working[self.categorical_features].copy()
            for col in self.categorical_features:
                cats[col] = normalize_categorical(cats[col])
            cat_df = pd.get_dummies(cats, columns=self.categorical_features, drop_first=False, dtype=float)
        encoded = pd.concat([working[self.numeric_features], cat_df], axis=1).fillna(0.0)
        encoded.columns = [
            "_".join(filter(None, "".join(ch if ch.isalnum() else "_" for ch in name).split("_"))) or "feature"
            for name in encoded.columns
        ]
        return encoded

# 5. Optuna hyperparameter optimization -----------------------------------------
class OptunaQuantileOptimizer:
    """Optuna-based hyperparameter optimizer for quantile regression."""

    def __init__(
        self,
        n_trials: int = OPTUNA_TRIALS,
        random_state: int = RANDOM_STATE,
    ):
        self.n_trials = n_trials
        self.random_state = random_state
        self.best_params: Dict[float, Dict[str, Any]] = {}

    def optimize(
        self,
        features: pd.DataFrame,
        target: np.ndarray,
        folds: List[Tuple[np.ndarray, np.ndarray]],
        alpha: float = 0.5,
    ) -> Tuple[Dict[str, Any], int]:
        """Optimize hyperparameters for a specific quantile."""

        def objective(trial: optuna.Trial) -> float:
            params = {
                "max_depth": trial.suggest_int("max_depth", *OPTUNA_PARAM_SPACE["max_depth"]),
                "num_leaves": trial.suggest_int("num_leaves", *OPTUNA_PARAM_SPACE["num_leaves"]),
                "min_child_samples": trial.suggest_int(
                    "min_child_samples", *OPTUNA_PARAM_SPACE["min_child_samples"]
                ),
                "subsample": trial.suggest_float("subsample", *OPTUNA_PARAM_SPACE["subsample"]),
                "colsample_bytree": trial.suggest_float(
                    "colsample_bytree", *OPTUNA_PARAM_SPACE["colsample_bytree"]
                ),
                "reg_alpha": trial.suggest_float("reg_alpha", *OPTUNA_PARAM_SPACE["reg_alpha"]),
                "reg_lambda": trial.suggest_float("reg_lambda", *OPTUNA_PARAM_SPACE["reg_lambda"]),
                "learning_rate": trial.suggest_float(
                    "learning_rate", *OPTUNA_PARAM_SPACE["learning_rate"], log=True
                ),
                "min_gain_to_split": trial.suggest_float(
                    "min_gain_to_split", *OPTUNA_PARAM_SPACE["min_gain_to_split"]
                ),
                "n_estimators": MAX_LGB_TREES,
                "random_state": self.random_state,
                "verbosity": -1,
            }

            fold_mae = []
            fold_iters = []

            for train_idx, val_idx in folds:
                train_X, val_X = features.iloc[train_idx], features.iloc[val_idx]
                train_y, val_y = target[train_idx], target[val_idx]

                model = lgb.LGBMRegressor(objective="quantile", alpha=float(alpha), **params)
                model.fit(
                    train_X,
                    train_y,
                    eval_set=[(val_X, val_y)],
                    eval_metric="mae",
                    callbacks=[lgb.early_stopping(EARLY_STOPPING_ROUNDS, verbose=False)],
                )

                preds = model.predict(val_X)
                fold_mae.append(float(np.mean(np.abs(preds - val_y))))
                fold_iters.append(model.best_iteration_ or params["n_estimators"])

            return float(np.mean(fold_mae))

        # Create study with TPE sampler
        sampler = TPESampler(seed=self.random_state)
        study = optuna.create_study(direction="minimize", sampler=sampler)
        study.optimize(objective, n_trials=self.n_trials, show_progress_bar=True)

        best_params = {k: v for k, v in study.best_trial.params.items()}
        best_params["random_state"] = self.random_state
        best_params["verbosity"] = -1

        # Estimate best iterations from top trials
        best_iter = max(80, int(np.mean([t.user_attrs.get("best_iter", 500) for t in study.best_trials[:5]])))

        self.best_params[alpha] = best_params
        return best_params, best_iter


def random_search_fallback(
    features: pd.DataFrame,
    target: np.ndarray,
    folds: List[Tuple[np.ndarray, np.ndarray]],
    alpha: float,
    n_trials: int,
    random_state: int,
) -> Tuple[Dict[str, Any], int]:
    """Fallback random search if Optuna is not available."""
    best_score = math.inf
    best_params = None
    best_iter = MAX_LGB_TREES

    for _ in range(n_trials):
        params = {
            "max_depth": random.choice(LGB_PARAM_GRID["max_depth"]),
            "num_leaves": random.choice(LGB_PARAM_GRID["num_leaves"]),
            "min_child_samples": random.choice(LGB_PARAM_GRID["min_child_samples"]),
            "subsample": random.choice(LGB_PARAM_GRID["subsample"]),
            "colsample_bytree": random.choice(LGB_PARAM_GRID["colsample_bytree"]),
            "reg_alpha": random.choice(LGB_PARAM_GRID["reg_alpha"]),
            "reg_lambda": random.choice(LGB_PARAM_GRID["reg_lambda"]),
            "learning_rate": random.choice(LGB_PARAM_GRID["learning_rate"]),
            "min_gain_to_split": random.choice(LGB_PARAM_GRID["min_gain_to_split"]),
            "n_estimators": MAX_LGB_TREES,
            "random_state": random_state,
            "verbosity": -1,
        }

        fold_mae = []
        fold_iters = []

        for train_idx, val_idx in folds:
            train_X, val_X = features.iloc[train_idx], features.iloc[val_idx]
            train_y, val_y = target[train_idx], target[val_idx]

            model = lgb.LGBMRegressor(objective="quantile", alpha=float(alpha), **params)
            model.fit(
                train_X,
                train_y,
                eval_set=[(val_X, val_y)],
                eval_metric="mae",
                callbacks=[lgb.early_stopping(EARLY_STOPPING_ROUNDS, verbose=False)],
            )

            preds = model.predict(val_X)
            fold_mae.append(float(np.mean(np.abs(preds - val_y))))
            fold_iters.append(model.best_iteration_ or params["n_estimators"])

        avg_mae = float(np.mean(fold_mae))
        avg_iter = int(np.mean(fold_iters)) if fold_iters else MAX_LGB_TREES

        if avg_mae < best_score:
            best_score = avg_mae
            best_params = {k: v for k, v in params.items() if k != "n_estimators"}
            best_iter = max(80, avg_iter)

    if best_params is None:
        best_params = {
            "max_depth": 6,
            "num_leaves": 31,
            "min_child_samples": 40,
            "subsample": 0.8,
            "colsample_bytree": 0.8,
            "reg_alpha": 0.5,
            "reg_lambda": 1.0,
            "learning_rate": 0.03,
            "min_gain_to_split": 0.0,
            "random_state": random_state,
            "verbosity": -1,
        }
        best_iter = 800

    return best_params, best_iter

# 6. Enhanced LightGBM quantile model -------------------------------------------
class EnhancedLightGBMQuantileModel:
    """LightGBM quantile model with Optuna tuning and quantile-specific parameters."""

    DEFAULT_ALPHAS = (0.25, 0.5, 0.75)

    def __init__(
        self,
        categorical_features: List[str],
        numeric_features: List[str],
        target_column: str,
        random_state: int,
        use_log_target: bool = USE_LOG_TARGET,
        n_trials: int = OPTUNA_TRIALS,
    ):
        self.alphas = self.DEFAULT_ALPHAS
        self.categorical_features = categorical_features
        self.numeric_features = numeric_features
        self.target_column = target_column
        self.random_state = random_state
        self.use_log_target = use_log_target
        self.n_trials = n_trials
        self.encoder: Optional[QuantileFeatureEncoder] = None
        self.models: Dict[float, lgb.LGBMRegressor] = {}
        self.training_metrics: Dict[str, float] = {}
        self.calibration_state: Optional[Dict] = None
        self.best_params: Dict[float, Dict[str, Any]] = {}
        self.best_iterations: Dict[float, int] = {}
        self.is_fitted = False
        self.feature_importances: Optional[pd.DataFrame] = None

    def fit(self, df: pd.DataFrame, folds: List[Tuple[np.ndarray, np.ndarray]]) -> Dict[str, float]:
        if len(df) < MIN_ROWS:
            print(f"LightGBM: insufficient rows ({len(df)}).")
            return {}

        self.encoder = QuantileFeatureEncoder(self.categorical_features, self.numeric_features)
        features = self.encoder.fit_transform(df)
        target = df[self.target_column].to_numpy(dtype=float)

        print(f"\nTraining with {len(features.columns)} features on {len(df)} samples")
        print(f"Using {'Optuna' if OPTUNA_AVAILABLE else 'Random Search'} optimization ({self.n_trials} trials)")
        print(f"Log-transformed target: {self.use_log_target}")

        # Optimize and train for each quantile separately
        self.models = {}
        self.training_metrics = {}
        importance_accumulator = {}

        for alpha in self.alphas:
            print(f"\n--- Optimizing for quantile α={alpha} ---")

            if OPTUNA_AVAILABLE:
                optimizer = OptunaQuantileOptimizer(n_trials=self.n_trials, random_state=self.random_state)
                best_params, best_iter = optimizer.optimize(features, target, folds, alpha=alpha)
            else:
                best_params, best_iter = random_search_fallback(
                    features, target, folds, alpha, self.n_trials, self.random_state
                )

            self.best_params[alpha] = best_params
            self.best_iterations[alpha] = best_iter

            # Train final model
            model = lgb.LGBMRegressor(
                objective="quantile",
                alpha=float(alpha),
                n_estimators=best_iter,
                **best_params,
            )
            model.fit(features, target)

            # Calculate training MAE
            preds = model.predict(features)
            mae = float(np.mean(np.abs(preds - target)))
            self.training_metrics[f"mae_q{int(alpha * 100)}"] = mae
            self.models[float(alpha)] = model

            # Accumulate feature importances
            for fname, imp in zip(features.columns, model.feature_importances_):
                if fname not in importance_accumulator:
                    importance_accumulator[fname] = []
                importance_accumulator[fname].append(imp)

            print(f"  Best params: lr={best_params.get('learning_rate', 'N/A'):.4f}, "
                  f"depth={best_params.get('max_depth', 'N/A')}, "
                  f"leaves={best_params.get('num_leaves', 'N/A')}")
            print(f"  Best iterations: {best_iter}, Training MAE: {mae:.2f}")

        # Calculate average feature importance
        self.feature_importances = pd.DataFrame({
            "feature": list(importance_accumulator.keys()),
            "importance": [np.mean(v) for v in importance_accumulator.values()],
        }).sort_values("importance", ascending=False)

        self.is_fitted = bool(self.models)
        return self.training_metrics

    def update_calibration(self, state: Optional[Dict]) -> None:
        self.calibration_state = state or None

    def _resolve_bucket_key(self, project_features: pd.DataFrame) -> str:
        if not self.calibration_state:
            return "global"
        bucket_feature = self.calibration_state.get("bucket_feature")
        bucket_edges = self.calibration_state.get("bucket_edges")
        if not bucket_feature or bucket_edges is None:
            return "global"
        if bucket_feature not in project_features.columns:
            return "global"
        value = project_features.iloc[0].get(bucket_feature)
        if pd.isna(value):
            return "global"
        boundaries = np.array(bucket_edges)[1:-1]
        if boundaries.size == 0:
            return "global"
        idx = int(np.digitize([value], boundaries, right=True)[0])
        return f"bucket_{idx}"

    def _apply_quantile_offset(self, label: str, value: float, bucket_key: str) -> float:
        if not self.calibration_state:
            return value
        offsets = self.calibration_state.get("offsets") or {}
        bucket_offsets = offsets.get(bucket_key, {})
        global_offsets = offsets.get("global", {})
        offset = bucket_offsets.get(label, global_offsets.get(label, 0.0))
        return value + (offset or 0.0)

    def _apply_width_scaling(
        self, results: Dict[str, Optional[float]], bucket_key: str
    ) -> Dict[str, Optional[float]]:
        if not self.calibration_state:
            return results
        scale_map = self.calibration_state.get("scale_factors") or {}
        scale = scale_map.get(bucket_key, scale_map.get("global", 1.0))
        if scale is None or not np.isfinite(scale):
            return results
        p25, p50, p75 = results.get("p25"), results.get("p50"), results.get("p75")
        if None in (p25, p50, p75) or p75 <= p25:
            return results
        half_width = max((p75 - p25) / 2.0, 1.0)
        new_half = float(np.clip(half_width * scale, 1.0, 400.0))
        results["p25"] = int(round(max(0.0, p50 - new_half)))
        results["p75"] = int(round(max(results["p25"] + 1.0, p50 + new_half)))
        return ensure_monotonic(results)

    def predict_raw(self, project_features: pd.DataFrame) -> Dict[str, Optional[float]]:
        """Predict raw values (in log-space if use_log_target)."""
        if not self.is_fitted or self.encoder is None:
            return {"p25": None, "p50": None, "p75": None}
        encoded = self.encoder.transform(project_features)
        if encoded.empty:
            return {"p25": None, "p50": None, "p75": None}

        results: Dict[str, Optional[float]] = {"p25": None, "p50": None, "p75": None}
        for alpha, model in self.models.items():
            label = f"p{int(alpha * 100)}"
            raw_value = float(model.predict(encoded)[0])
            results[label] = raw_value
        return results

    def predict(
        self, project_features: pd.DataFrame, apply_calibration: bool = True
    ) -> Dict[str, Optional[int]]:
        """Predict with inverse transform and calibration."""
        MIN_GESTATION_FLOOR = MIN_GESTATION_DAYS  # Use global constant (5 days)

        if not self.is_fitted or self.encoder is None:
            return {"p25": None, "p50": None, "p75": None}

        encoded = self.encoder.transform(project_features)
        if encoded.empty:
            return {"p25": None, "p50": None, "p75": None}

        bucket_key = self._resolve_bucket_key(project_features)
        results: Dict[str, Optional[float]] = {"p25": None, "p50": None, "p75": None}

        for alpha, model in self.models.items():
            label = f"p{int(alpha * 100)}"
            raw_value = float(model.predict(encoded)[0])

            # Inverse log transform if needed
            if self.use_log_target:
                raw_value = np.expm1(raw_value)

            # Apply minimum floor after inverse transform
            raw_value = max(raw_value, MIN_GESTATION_FLOOR)

            if apply_calibration:
                raw_value = self._apply_quantile_offset(label, raw_value, bucket_key)

            results[label] = raw_value

        # Convert to integers and apply floor again
        results = {
            k: (int(round(max(v, MIN_GESTATION_FLOOR))) if v is not None else None)
            for k, v in results.items()
        }

        results = ensure_monotonic(results)

        if apply_calibration:
            results = self._apply_width_scaling(results, bucket_key)

        return results

    def get_feature_importance(self, top_n: int = 20) -> pd.DataFrame:
        """Get top N features by importance."""
        if self.feature_importances is None:
            return pd.DataFrame()
        return self.feature_importances.head(top_n)

# 7. Calibration & evaluation ---------------------------------------------------
def _build_bucket_edges(df: pd.DataFrame, feature: str, num_buckets: int) -> Optional[List[float]]:
    values = pd.to_numeric(df.get(feature), errors="coerce").dropna()
    if len(values) < num_buckets + 1:
        return None
    edges = np.quantile(values, np.linspace(0, 1, num_buckets + 1))
    edges = np.unique(edges)
    return edges.tolist() if len(edges) >= 3 else None


def _apply_offsets(
    preds: Dict[str, float], bucket_key: str, offsets: Dict[str, Dict[str, float]]
) -> Dict[str, float]:
    adjusted = preds.copy()
    bucket_offsets = offsets.get(bucket_key, {})
    global_offsets = offsets.get("global", {})
    for label in adjusted:
        adjusted[label] = adjusted[label] + bucket_offsets.get(label, global_offsets.get(label, 0.0))
    return adjusted


def calibrate_quantiles_conformal(
    model: EnhancedLightGBMQuantileModel,
    df: pd.DataFrame,
    target_column: str,
    bucket_feature: str,
    num_buckets: int,
    target_coverage: float,
) -> Optional[Dict]:
    """Calibration with conformal-style adjustments."""
    if df.empty:
        return None

    labels = [f"p{int(alpha * 100)}" for alpha in model.alphas]
    bucket_edges = _build_bucket_edges(df, bucket_feature, num_buckets)
    residuals: Dict[str, Dict[str, List[float]]] = {"global": {label: [] for label in labels}}
    coverage_records = []
    buckets_present = {"global"}

    for idx in range(len(df)):
        row = df.iloc[[idx]]
        truth = float(row[target_column].iloc[0])
        bucket_key = "global"

        if bucket_edges is not None:
            value = row.get(bucket_feature, pd.Series([np.nan])).iloc[0]
            if pd.notna(value):
                bucket_index = int(np.digitize([value], np.array(bucket_edges)[1:-1], right=True)[0])
                bucket_key = f"bucket_{bucket_index}"
                buckets_present.add(bucket_key)

        preds = model.predict(row, apply_calibration=False)
        coverage_records.append({"bucket": bucket_key, "truth": truth, "preds": preds})

        if bucket_key not in residuals:
            residuals[bucket_key] = {label: [] for label in labels}

        for label in labels:
            pred_value = preds.get(label)
            if pred_value is None:
                continue
            residuals["global"][label].append(truth - pred_value)
            residuals[bucket_key][label].append(truth - pred_value)

    # Calculate offsets using quantiles of residuals (conformal approach)
    offsets: Dict[str, Dict[str, float]] = {}
    for scope, label_map in residuals.items():
        offsets[scope] = {}
        for label, values in label_map.items():
            if values:
                alpha = int(label.replace("p", "")) / 100.0
                offsets[scope][label] = float(np.quantile(values, alpha))
            else:
                offsets[scope][label] = 0.0

    # Calculate adaptive scale factors based on coverage
    scale_factors: Dict[str, float] = {}
    for bucket in buckets_present:
        subset = [rec for rec in coverage_records if rec["bucket"] == bucket or bucket == "global"]
        if not subset:
            scale_factors[bucket] = 1.0
            continue

        coverage_flags = []
        widths = []
        for rec in subset:
            adjusted = _apply_offsets(rec["preds"], bucket if bucket != "global" else rec["bucket"], offsets)
            adjusted = ensure_monotonic({k: float(v) for k, v in adjusted.items()})
            width = max(adjusted["p75"] - adjusted["p25"], 1.0)
            widths.append(width)
            coverage_flags.append(1.0 if (adjusted["p25"] <= rec["truth"] <= adjusted["p75"]) else 0.0)

        coverage_rate = float(np.mean(coverage_flags)) if coverage_flags else 0.5
        scale = target_coverage / coverage_rate if coverage_rate > 0 else WIDTH_SCALE_CLIP[1]
        scale_factors[bucket] = float(np.clip(scale, *WIDTH_SCALE_CLIP))

    return {
        "offsets": offsets,
        "scale_factors": scale_factors,
        "bucket_feature": bucket_feature if bucket_edges is not None else None,
        "bucket_edges": bucket_edges,
    }


def evaluate_split(
    model: EnhancedLightGBMQuantileModel,
    df: pd.DataFrame,
    target_column: str,
    use_log_target: bool = False,
) -> Optional[Dict[str, float]]:
    """Evaluate model on a split, with proper inverse transform."""
    if df.empty:
        return None

    actual = df[target_column].to_numpy(dtype=float)
    mae = {"p25": [], "p50": [], "p75": []}
    widths, coverage = [], []

    for idx in range(len(df)):
        row = df.iloc[[idx]]
        preds = model.predict(row)
        p25, p50, p75 = preds.get("p25"), preds.get("p50"), preds.get("p75")

        if None in (p25, p50, p75):
            continue

        truth = actual[idx]
        mae["p25"].append(abs(p25 - truth))
        mae["p50"].append(abs(p50 - truth))
        mae["p75"].append(abs(p75 - truth))
        widths.append(max(p75 - p25, 0))
        coverage.append(1.0 if p25 <= truth <= p75 else 0.0)

    if not widths:
        return None

    span = max(actual) - min(actual) or 1.0
    return {
        "rows_evaluated": len(widths),
        "mae_p25": float(np.mean(mae["p25"])) if mae["p25"] else None,
        "mae_p50": float(np.mean(mae["p50"])) if mae["p50"] else None,
        "mae_p75": float(np.mean(mae["p75"])) if mae["p75"] else None,
        "coverage_p25_p75": float(np.mean(coverage)),
        "pinaw": float(np.mean(widths) / span),
    }


def clip_target(
    train_df: pd.DataFrame, other_frames: List[pd.DataFrame], quantiles: Tuple[float, float]
) -> Tuple[float, float]:
    low_q, high_q = train_df[TARGET_COLUMN].quantile(quantiles)
    train_df[CLIPPED_TARGET_COLUMN] = train_df[TARGET_COLUMN].clip(low_q, high_q)
    for frame in other_frames:
        frame[CLIPPED_TARGET_COLUMN] = frame[TARGET_COLUMN].clip(low_q, high_q)
    return float(low_q), float(high_q)


def add_log_target(train_df: pd.DataFrame, other_frames: List[pd.DataFrame]) -> None:
    """Add log-transformed target column."""
    train_df[LOG_TARGET_COLUMN] = np.log1p(train_df[CLIPPED_TARGET_COLUMN])
    for frame in other_frames:
        frame[LOG_TARGET_COLUMN] = np.log1p(frame[CLIPPED_TARGET_COLUMN])


def export_model_metadata(
    name: str,
    model: EnhancedLightGBMQuantileModel,
    train_metrics: Dict[str, float],
    test_metrics: Optional[Dict[str, float]],
    calibration_offsets: Optional[Dict],
    dataset_hash_value: str,
    clip_range: Tuple[float, float],
    config: Dict[str, Any],
) -> Tuple[Dict, Path, Path]:
    timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    artifact_path = OUTPUT_DIR / f"{name}_quantiles_{MODEL_VERSION}_{timestamp}.pkl"

    with artifact_path.open("wb") as handle:
        pickle.dump(model, handle)

    metadata = {
        "model_version": MODEL_VERSION,
        "artifact_path": str(artifact_path),
        "snapshot_path": str(DATASET_PATH),
        "generated_at_utc": timestamp,
        "config": config,
        "clip_range": clip_range,
        "dataset_hash": dataset_hash_value,
        "train_metrics": train_metrics,
        "test_metrics": test_metrics,
        "calibration_offsets": calibration_offsets,
        "best_params": {str(k): v for k, v in model.best_params.items()},
        "best_iterations": {str(k): v for k, v in model.best_iterations.items()},
        "feature_context": {
            "categorical_features": ONE_HOT_CATEGORICAL,
            "numeric_features": NUMERIC_FEATURES,
            "target_encoded_columns": TARGET_ENCODE_COLUMNS,
        },
        "top_features": model.get_feature_importance(20).to_dict("records"),
    }

    metadata_path = OUTPUT_DIR / f"{name}_quantiles_training_{timestamp}.json"
    metadata_path.write_text(json.dumps(metadata, indent=2))

    return metadata, artifact_path, metadata_path

# 8. Main execution -------------------------------------------------------------
if __name__ == "__main__":
    print("=" * 70)
    print("Quantile Regression v3 - Enhanced LightGBM")
    print("=" * 70)

    # Load data
    raw_dataset = pd.read_csv(DATASET_PATH)
    initial_rows = len(raw_dataset)
    print(f"\nLoaded {initial_rows} rows from {DATASET_PATH}")

    # Filter by gestation range
    raw_dataset = raw_dataset[
        raw_dataset[TARGET_COLUMN].between(MIN_GESTATION_DAYS, MAX_GESTATION_DAYS)
    ].copy()
    print(f"After gestation range filter [{MIN_GESTATION_DAYS}, {MAX_GESTATION_DAYS}]: {len(raw_dataset)} rows")

    # Filter censored observations (optional)
    if FILTER_CENSORED:
        pre_filter = len(raw_dataset)
        raw_dataset = raw_dataset[raw_dataset[TARGET_COLUMN] < CENSORING_THRESHOLD].copy()
        print(f"After censoring filter (< {CENSORING_THRESHOLD}): {len(raw_dataset)} rows "
              f"(removed {pre_filter - len(raw_dataset)} potentially censored)")

    raw_dataset[TARGET_COLUMN] = raw_dataset[TARGET_COLUMN].round().astype(int)
    base_dataset_hash = dataset_hash(raw_dataset)

    # Target distribution
    print(f"\nTarget distribution:")
    print(f"  Min: {raw_dataset[TARGET_COLUMN].min()}")
    print(f"  Max: {raw_dataset[TARGET_COLUMN].max()}")
    print(f"  Mean: {raw_dataset[TARGET_COLUMN].mean():.1f}")
    print(f"  Median: {raw_dataset[TARGET_COLUMN].median():.1f}")
    print(f"  Std: {raw_dataset[TARGET_COLUMN].std():.1f}")

    # Train/validation split
    train_df, val_df = train_val_split(raw_dataset, VAL_SIZE, RANDOM_STATE)
    print(f"\nRows → train={len(train_df)} | validation={len(val_df)}")

    # Feature engineering
    feature_builder = EnhancedFeatureBuilder("date_created")
    train_df = feature_builder.transform(train_df)
    val_df = feature_builder.transform(val_df)

    # Temporal CV folds with purge gap
    cv_folds = temporal_folds_with_purge(train_df, CV_FOLDS, "date_created", RANDOM_STATE, PURGE_GAP_DAYS)
    print(f"Created {len(cv_folds)} CV folds with {PURGE_GAP_DAYS}-day purge gap")

    # Account stats (OOF)
    account_builder = AccountStatsBuilder(TARGET_COLUMN)
    train_df = account_builder.fit_transform(train_df, cv_folds)
    val_df = account_builder.transform(val_df)

    # Target encoding (OOF)
    target_encoder = OOFTargetEncoder(TARGET_ENCODE_COLUMNS, TARGET_COLUMN)
    train_df = target_encoder.fit_transform(train_df, cv_folds)
    val_df = target_encoder.transform(val_df)

    # Clip target
    clip_range = clip_target(train_df, [val_df], CLIP_QUANTILES)
    print(f"Target clipped to range: [{clip_range[0]:.1f}, {clip_range[1]:.1f}]")

    # Add log target
    if USE_LOG_TARGET:
        add_log_target(train_df, [val_df])
        model_target = LOG_TARGET_COLUMN
        print("Using log-transformed target")
    else:
        model_target = CLIPPED_TARGET_COLUMN

    # Re-add interaction features now that we have target encodings
    train_df = feature_builder._add_interaction_features(train_df)
    val_df = feature_builder._add_interaction_features(val_df)

    # Initialize and train model
    lgb_model = EnhancedLightGBMQuantileModel(
        categorical_features=ONE_HOT_CATEGORICAL,
        numeric_features=NUMERIC_FEATURES,
        target_column=model_target,
        random_state=RANDOM_STATE,
        use_log_target=USE_LOG_TARGET,
        n_trials=OPTUNA_TRIALS,
    )

    lgb_training_metrics = lgb_model.fit(train_df, cv_folds)
    if not lgb_model.is_fitted:
        raise RuntimeError("LightGBM failed to fit.")

    # Feature importance
    print("\n--- Top 15 Feature Importances ---")
    display(lgb_model.get_feature_importance(15))

    # Calibration/test split
    calibration_df, test_df = train_val_split(val_df, CALIBRATION_FRACTION, RANDOM_STATE + 11)
    print(f"\nCalibration rows={len(calibration_df)} | Test rows={len(test_df)}")

    # Calibrate
    lgb_calibration = calibrate_quantiles_conformal(
        lgb_model,
        calibration_df,
        target_column=TARGET_COLUMN,  # Use original target for calibration
        bucket_feature=CALIBRATION_BUCKET_FEATURE,
        num_buckets=CALIBRATION_BUCKETS,
        target_coverage=TARGET_COVERAGE,
    )
    lgb_model.update_calibration(lgb_calibration)

    # Evaluate on test set
    lgb_test_metrics = evaluate_split(lgb_model, test_df, TARGET_COLUMN, USE_LOG_TARGET)

    # Configuration summary
    config = {
        "lookback_days": LOOKBACK_DAYS,
        "val_size": VAL_SIZE,
        "calibration_fraction": CALIBRATION_FRACTION,
        "min_rows": MIN_ROWS,
        "min_gestation_days": MIN_GESTATION_DAYS,
        "max_gestation_days": MAX_GESTATION_DAYS,
        "censoring_threshold": CENSORING_THRESHOLD,
        "filter_censored": FILTER_CENSORED,
        "clip_quantiles": CLIP_QUANTILES,
        "category_min_frequency": CATEGORY_MIN_FREQUENCY,
        "use_log_target": USE_LOG_TARGET,
        "optuna_trials": OPTUNA_TRIALS,
        "cv_folds": CV_FOLDS,
        "purge_gap_days": PURGE_GAP_DAYS,
    }

    # Export
    lgb_metadata, lgb_artifact, lgb_metadata_path = export_model_metadata(
        "lightgbm",
        lgb_model,
        lgb_training_metrics,
        lgb_test_metrics,
        lgb_calibration,
        base_dataset_hash,
        clip_range,
        config,
    )

    # Results summary
    print("\n" + "=" * 70)
    print("RESULTS SUMMARY")
    print("=" * 70)

    comparison = pd.DataFrame([{"model": "lightgbm_v3", **(lgb_test_metrics or {})}])
    display(comparison)

    # Sample predictions
    if not test_df.empty:
        print("\n--- Sample Predictions ---")
        samples = test_df.sample(min(5, len(test_df)), random_state=0)
        for idx in range(len(samples)):
            row = samples.iloc[[idx]]
            preds = lgb_model.predict(row)
            actual = row[TARGET_COLUMN].iloc[0]
            account = row["account"].iloc[0][:40]
            print(f"{account}: Actual={actual}, Pred={preds}")

    print(f"\nArtifact saved: {lgb_artifact}")
    print(f"Metadata saved: {lgb_metadata_path}")

    # Compare with baseline (if available)
    print("\n" + "=" * 70)
    print("IMPROVEMENT TARGETS")
    print("=" * 70)
    print("Previous v2 results (from screenshot):")
    print("  MAE P25: 105.4, MAE P50: 109.17, MAE P75: 149.26")
    print("  Coverage: 50.6%, PINAW: 0.227")
    print("\nCurrent v3 results:")
    if lgb_test_metrics:
        print(f"  MAE P25: {lgb_test_metrics.get('mae_p25', 'N/A'):.2f}")
        print(f"  MAE P50: {lgb_test_metrics.get('mae_p50', 'N/A'):.2f}")
        print(f"  MAE P75: {lgb_test_metrics.get('mae_p75', 'N/A'):.2f}")
        print(f"  Coverage: {lgb_test_metrics.get('coverage_p25_p75', 0) * 100:.1f}%")
        print(f"  PINAW: {lgb_test_metrics.get('pinaw', 'N/A'):.3f}")